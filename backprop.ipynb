{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PS7 (CLEAN)",
      "provenance": [],
      "collapsed_sections": [
        "bhcnqfawZhmx",
        "Ooh3KTatYqgl",
        "Ty4jZ23sPvVA",
        "PwXLKsE9QQjt",
        "PJcDH94VQ3iV"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Croftc/backprop/blob/main/backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhcnqfawZhmx"
      },
      "source": [
        "# Problem Set 7: Backpropagation\n",
        "# CMSC 422, Fall 2020\n",
        "# Due Nov 20 at 11:59pm\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1914/1*F9capAHwl_rz2-Q8z511WQ.jpeg\" alt=\"meme\" width=\"500px\"/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooh3KTatYqgl"
      },
      "source": [
        "# Instructions\n",
        "In this problem set you will implement backpropagation for a set of different neural network architectures.\n",
        "There is some code provided for you here, and you will write your implementations in the places marked with __```#TODO: Your Code Here```__. You may add helper functions if you feel you need to.\n",
        "\n",
        "__Analysis Questions:__ In addition to Python programming, each problem will contain some analysis questions (under __Analysis__). These are meant to ensure you understand your results, and will be manually graded on Gradescope.\n",
        "\n",
        "__Submission:__ download this notebook as a `.ipynb` file and submit it to Gradescope. This assignment will be partially autograded so follow instructions closely.  \n",
        " \n",
        "- Make sure your plots are visible when downloading the notebook, otherwise they won't appear on Gradescope. \n",
        "- Make sure your code cells are not throwing exceptions.\n",
        "- Please do not import any packages other than what has already been imported here. You may be penalized for doing so.\n",
        "- Lastly, the autograder times out after 40 minutes, so make sure your implementation is relativly efficient (e.g. by using numpy for matrix operations). Our implementation took a little over 10 minutes to test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOtWraGuaWzc"
      },
      "source": [
        "# Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noBuJ-fIZJ0m"
      },
      "source": [
        "## Problem 1 (25 Points)\n",
        "We'll begin with the simplest possible network (a single layer perceptron). It has a single input feature that we call $x$. This is the activation of the single node of the input layer. This is connected to a single output node, which has a weight, $w$. We also have a bias term, so the activation of the output unit is $a = wx + b$. This network will be used to solve a linear regression problem. So, if we are given an input pair of $(x,y)$, we want to minimize the squared loss: \n",
        "\n",
        "$$L(x,y) = (a-y)^2 = (wx + b - y)^2$$\n",
        "\n",
        "To do this, you will need to randomly initialize the weight and bias and then perform gradient descent.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1Qz8jJaPXbVzoL44Nd4nbQgFHOA8G_XyI&sz=w1000\" alt=\"net1\" width=\"150px\"/>\n",
        "<br>\n",
        "<i>Figure 1: Network for Problem 1</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "The gradient of the loss is computed using a training set containing pairs, $(x_1, y_1), (x_2, y_2), ... (x_n, y_n)$. We have:\n",
        "\n",
        "$$\\nabla L = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{\\partial L}{\\partial w}(x_i, y_i), \\frac{\\partial L}{\\partial b}(x_i, y_i)  \\right)$$\n",
        "\n",
        "If we denote $\\theta = (w,b)$ as a vector containing all the parameters of the network, we perform gradient\n",
        "descent with the update:\n",
        "\n",
        "$$\\theta^k = \\theta^{k-1} - \\eta \\nabla L$$\n",
        "\n",
        "Here $\\eta$ is the learning rate, and $\\theta^k$ denotes a vector of $(w,b)$ after the $k$'th iteration of gradient descent. Do not mistake $\\eta$ (the learning rate) for $n$ (the number of data points).\n",
        "We provide you with a routine to generate training data. This has the form: \n",
        "\n",
        "```simplest_training_data(n)```  \n",
        "  \n",
        "This just generates $n$ random training points on the line $y = 3x + 2$, with a little Gaussian noise added to the points.  \n",
        "You need to write a routine with the form: \n",
        "\n",
        "```simplest_training(n, k, eta)```\n",
        "\n",
        "Here $n$ indicates the number of points in the training set (you can call `simplest_training_data` to get the training data), $k$ indicates the total number of iterations that you will use in training, and $\\eta$ is the learning rate.  To initialize the weights in your network, we suggest that you initialize $w$ with a Gaussian random variable with mean 0 and variance of 1, and that you initialize $b = 0$.  \n",
        "You also need to write a routine of the form: \n",
        "\n",
        "```simplest_testing(theta, x)```\n",
        "\n",
        "This routine applies the network, using the parameters in theta, to the input values in the vector $x$, and returns a vector of results in $y$.\n",
        "After training, the network should learn $w$ and $b$ values that are similar to those used to train the network.  So you can test your network by looking at the learned $w$ and $b$ values.  Or you can use the testing algorithm to see if the network computes appropriate $y$ values.  In testing, you may find that if you use too big a value for $\\eta$ the network will not converge to anything meaningful.  If you use a value of $k$ that is too small, it won't have time to converge to a good solution.\n",
        "We run our algorithm with $n = 30, k = 10,000, \\eta = .02$.   When we test using $x = (0, 1, ..., 9)$ we get the result:\n",
        "  \n",
        "```\n",
        "1.9504  4.9666  7.9828  10.9990  14.0152  17.0314  20.0476  23.0638  26.0800  29.0962\n",
        "```\n",
        "\n",
        "These points fit the line $y = 3x + 2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZsEHnFFs01Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736174ad-49ca-4f2d-a99a-ff86fbcf96b8"
      },
      "source": [
        "import numpy as np\n",
        "import math as m\n",
        "import sys\n",
        "\n",
        "# derivitive of the loss wrt w\n",
        "def d_w(x,y,w,b):\n",
        "  return (2 * pow(x,2) * w) + (2 * b * x) - (2 * x * y)\n",
        "\n",
        "# derivitive of the loss wrt b\n",
        "def d_b(x,y,w,b):\n",
        "  return (2 * b) + (2 * w * x) - (2 * y)\n",
        "\n",
        "###Problem 1\n",
        "###Provided function to create training data\n",
        "def simplest_training_data(n):\n",
        "    m = 3\n",
        "    b = 2\n",
        "    x = np.random.uniform(0,1,n)\n",
        "    y = m*x+b+0.3*np.random.normal(0,1,n)\n",
        "    return (x,y)\n",
        "\n",
        "def simplest_training(n, k, eta):\n",
        "  x,y = simplest_training_data(n)\n",
        "  w = np.random.normal()\n",
        "  b = 0\n",
        "  theta = np.asarray([w,b])\n",
        "  gl_wrt_w = 0\n",
        "  gl_wrt_b = 0\n",
        "  for j in range(k):\n",
        "    gl_wrt_w = 0\n",
        "    gl_wrt_b = 0\n",
        "    w = theta[0]\n",
        "    b = theta[1]\n",
        "    for i in range(n):\n",
        "      gl_wrt_w += d_w(x[i],y[i],w,b)\n",
        "      gl_wrt_b += d_b(x[i],y[i],w,b)\n",
        "    nw = gl_wrt_w/n\n",
        "    nb = gl_wrt_b/n\n",
        "    gl = np.asarray([nw,nb])\n",
        "    theta = theta - (eta * gl)\n",
        "  return theta \n",
        "\n",
        "\n",
        "def simplest_testing(theta, X):\n",
        "  Y = []\n",
        "  for x in X:\n",
        "    Y.append((theta[0]*x) + theta[1])\n",
        "  return Y\n",
        "\n",
        "\n",
        "theta = simplest_training(30, 30000, 0.02)\n",
        "\n",
        "X = [i for i in range(-10,11)]\n",
        "print(\"X: \", X)\n",
        "Y = [((3*x)+2) for x in X]\n",
        "print(\"Y: \", Y)\n",
        "Y_hats = simplest_testing(theta,X)\n",
        "Y_hats = [round(x,2) for x in Y_hats]\n",
        "print(\"Predictions: \", Y_hats)\n",
        "print(\"Errors: \", [round(pow((Y_hats[i]-Y[i]),2),2) for i in range(21)])\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:  [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Y:  [-28, -25, -22, -19, -16, -13, -10, -7, -4, -1, 2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32]\n",
            "Predictions:  [-31.08, -27.79, -24.49, -21.19, -17.9, -14.6, -11.3, -8.01, -4.71, -1.41, 1.89, 5.18, 8.48, 11.78, 15.07, 18.37, 21.67, 24.96, 28.26, 31.56, 34.85]\n",
            "Errors:  [9.49, 7.78, 6.2, 4.8, 3.61, 2.56, 1.69, 1.02, 0.5, 0.17, 0.01, 0.03, 0.23, 0.61, 1.14, 1.88, 2.79, 3.84, 5.11, 6.55, 8.12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBZDJthCOAOw"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Test your algorithm using the test mentioned above or any other test you choose. Provide a description of your test (including the values chosen/used) and your results inside this cell.  \n",
        "\n",
        "I used the above mentioned test, and compared the predicted values of Y to the actual values computed by y = 3x+2. I chose to test using all integers between -10 and 10 as values of x. I also printed out the loss for each of these predictions. The results show that the values predicted are very close to the actual values of the function. The squared loss being so low for all of the values also confirms the trained model performs well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KBU60MFa84o"
      },
      "source": [
        "## Problem 2 (35 Points)\n",
        "You will now create a network that is a little more complicated. It still contains just an input and an output layer, with no hidden layers. But it now has a nonlinearity along with a cross-entropy loss, so that we can use it for classification.\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1UkNx6-HghYRsjrXbqB6jN04VUsA-_RKp&sz=w1000\" alt=\"net2\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 2: Network for Problem 2</i>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "The network has two inputs, $x_1$ and $x_2$.  These are connected with two weights to a single output unit.  If we let $z = w_1x_1 + w_2x_2 + b$, the output unit will have an activation of $a = \\sigma(z)$, where $\\sigma(z)$ represents the sigmoid function:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "We can interpret the output as giving the probability that the input belongs to class 1. If the probability is low, then the input probably belongs to class 0. Hint: the derivative of the sigmoid is given by:\n",
        "\n",
        "$$\\frac{d\\sigma}{dz} = \\sigma(z)(1-\\sigma(z))$$\n",
        "\n",
        "In training the network, you will use the cross-entropy loss. In this case, the cross entropy loss will be:\n",
        "\n",
        "$$L_{CE}(x,y) = -(y\\log{a} + (1-y)\\log{(1-a)})$$\n",
        "\n",
        "If $y = 1$, this is just the negative log of what the network predicts for the probability that the input belongs to class 1.  If $y = 0$, it is the negative log of the probability that the input belongs to class 0.\n",
        "We provide you with a routine to generate training data. This has the form:\n",
        "\n",
        "```single_layer_training_data(trainset)```  \n",
        "    \n",
        "which returns $X$ and $y$.\n",
        "This provides two different training sets.  When the input, trainset, is 1, the function produces a simple, linearly separable training set.  Half the points are near $(0,0)$ and half are near $(10,10)$.  $X$ is a matrix in which each row contains one of these points, so it is $n \\times 2$, where $n$ is the number of points.  $y$ is a vector of class labels, which have the value 1 for the points near $(0,0)$ and 0 for the points near $(10,10)$.\n",
        "\n",
        "When trainset is 2, we generate a different training set that is not linearly separable, but that corresponds to the Xor problem.  Points from class 1 are either near $(0,0)$ or $(10,10)$, while points in class 0 are near either $(10,0)$ or $(0,10)$.\n",
        "\n",
        "You will need to implement two routines.  \n",
        "The first is: \n",
        "\n",
        "```single_layer_training(k, eta, trainset)```  \n",
        "  \n",
        "As before, $k$ will indicate the number of iterations of gradient descent and eta gives the learning rate.  trainset indicates which training set to use, 1 or 2.  You will train the network using the same gradient descent approach as in the previous problem.  As before, we suggest that you initialize weights using random values chosen from a Gaussian distribution with zero mean, and that you initialize bias at 0.  \n",
        "\n",
        "You will also implement a test routine: \n",
        "\n",
        "```single_layer_testing(theta, X)```  \n",
        "  \n",
        "This takes in the network parameters and a matrix, $X$, of the form returned by single\\_layer\\_training\\_data.  It returns a vector of the output values the network computes.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is NOT the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sLnCceZtHFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b30a0e54-39ee-483a-d32b-2bceb0a610bc"
      },
      "source": [
        "###Problem 2\n",
        "\n",
        "def sigma(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "###Provided function to create training data\n",
        "def single_layer_training_data(trainset):\n",
        "    n = 10\n",
        "    if trainset == 1:\n",
        "    # Linearly separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(n), np.zeros(n)),axis=0)\n",
        "\n",
        "    elif trainset == 2:\n",
        "        # Not Linearly Separable\n",
        "        X = np.concatenate((np.random.normal((0,0),1,(n,2)), np.random.normal((10,10),1,(n,2)), np.random.normal((10,0),1,(n,2)), np.random.normal((0,10),1,(n,2))),axis=0)\n",
        "        y = np.concatenate((np.ones(2*n), np.zeros(2*n)), axis=0)\n",
        "\n",
        "    else:\n",
        "        print (\"function single_layer_training_data undefined for input\", trainset)\n",
        "        sys.exit()\n",
        "\n",
        "    return (X,y)\n",
        "\n",
        "def single_layer_training(k, eta, trainset):\n",
        "  X,Y = single_layer_training_data(trainset)\n",
        "  n = len(X)\n",
        "  w1 = np.random.normal()\n",
        "  w2 = np.random.normal()\n",
        "  b = 0\n",
        "  theta = np.asarray([w1,w2,b])\n",
        "  \n",
        "  for j in range(k):\n",
        "\n",
        "    # intialize acc for this epoch\n",
        "    acc = 0\n",
        "\n",
        "    for i in range(n):\n",
        "      # correct label\n",
        "      y = Y[i]\n",
        "\n",
        "      # x_i with 1 appended on the end to get bias\n",
        "      x = np.asarray([X[i][0],X[i][1],1])\n",
        "      \n",
        "      # inner product current theta with x_i\n",
        "      z = np.dot(theta,x)\n",
        "\n",
        "      # apply sigmoid\n",
        "      a = sigma(z)\n",
        "\n",
        "      # add these up so we can average\n",
        "      acc += (y-a)*x\n",
        "\n",
        "    # get the average\n",
        "    avg = acc/n\n",
        "\n",
        "    # update theta\n",
        "    theta = theta + (eta * avg)\n",
        "  return theta \n",
        "\n",
        "def single_layer_testing(theta, X):\n",
        "  Y = []\n",
        "  n = len(X)\n",
        "  for i in range(n):\n",
        "    x = np.asarray([X[i][0],X[i][1],1])\n",
        "    y_hat = sigma(np.dot(theta,x))\n",
        "    Y.append(y_hat)\n",
        "  return Y\n",
        "\n",
        "X,Y = single_layer_training_data(1)\n",
        "theta = single_layer_training(1000, .05, 1)\n",
        "Y1 = single_layer_testing(theta, X)\n",
        "Yhat = []\n",
        "C = []\n",
        "v0 = np.asarray([0,0])\n",
        "v1 = np.asarray([10,10])\n",
        "\n",
        "for y in Y1:\n",
        "  if y < .5:\n",
        "    Yhat.append(0)\n",
        "  else:\n",
        "    Yhat.append(1)\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == Yhat[i]:\n",
        "    correct += 1\n",
        "\n",
        "print(Y1)\n",
        "print(correct, \" correct out of: \", len(Yhat))\n",
        "X,Y = single_layer_training_data(2)\n",
        "theta = single_layer_training(1000, .05, 2)\n",
        "Y2 = single_layer_testing(theta, X)\n",
        "\n",
        "for y in Y1:\n",
        "  if y < .5:\n",
        "    Yhat.append(0)\n",
        "  else:\n",
        "    Yhat.append(1)\n",
        "\n",
        "correct = 0\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == Yhat[i]:\n",
        "    correct += 1\n",
        "\n",
        "print(Y2)\n",
        "print(correct, \" correct out of: \", len(Yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.9694644256635201, 0.9841503481370736, 0.9637619641774666, 0.9882226509578601, 0.9714665215413921, 0.969498612925003, 0.9536395445961839, 0.9484085444946239, 0.9470925582390929, 0.9891016045415524, 0.008818479849741483, 0.002500448755228931, 0.00355598196182553, 0.0038994471271408255, 0.002630620804451234, 0.003405986328503392, 0.005138907001187907, 0.0022752952974399213, 0.01088922453189835, 0.0009796025828524464]\n",
            "20  correct out of:  20\n",
            "[0.4728547268467772, 0.4740137231143443, 0.4769383195595161, 0.47439815777851574, 0.47463274172290704, 0.4842880599545467, 0.48603132907785707, 0.4784353263467278, 0.4742440066620653, 0.4806072361081382, 0.5198052071559165, 0.5235054676519755, 0.5185334635883061, 0.5241009991217221, 0.5277475092553786, 0.5219812893612311, 0.5287102457006306, 0.5258511878960972, 0.5249481555434822, 0.5244291287170123, 0.48746170101634484, 0.4907759095859866, 0.48222881932300393, 0.4856298772216834, 0.4799167951609707, 0.4935079071432734, 0.48817715021274444, 0.4866862342276615, 0.48869236717743636, 0.492905544527786, 0.5082101080335789, 0.520021504966891, 0.5201556921037257, 0.5210684903343956, 0.5112425746282475, 0.5110139529371214, 0.5162498889515429, 0.5062902696400292, 0.5174211794839413, 0.5117999000442269]\n",
            "20  correct out of:  40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty4jZ23sPvVA"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "Add a description (inside this cell) of tests of your code using the two trainset values.  You will need to figure out how many iterations of gradient descent to perform and the appropriate learning rate to get good results (mention these values and include the learned weights).  Do not use the data you used to train the network, call `single_layer_training_data` again to get fresh data for testing.  When trainset = 1, your network should assign a high probability of belonging to class 1 for points near $(0,0)$, and a low probability for points near $(10,10)$.  When trainset = 2, the data is not linearly separable, so you may find that your network has problems being able to separate.  Describe what happens in both cases.\n",
        "\n",
        "- I manually adjusted the values of $k$ and $\\eta$ and found that I could consistently get 100% accuracy (in trainset 1) with $k = 500$ and $\\eta$ = 0.1, but the probabilities given can be increased with a larger $k$. For trainset 2, with the same values of $eta$ and $k$ I could only manage to get 50% accuracy at best. This makes sense, because the points in trainset 2 are not linearly separable - in fact, they get separated into quarters, two of which get classified correctly (one in each class) and two get classified incorrectly (also 1 in each class), given this data, this particular model can't do any better on trainset 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBqMD3voqug8"
      },
      "source": [
        "## Problem 3 (40 Points)\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1lZQ1CnQUDD-kiyL-FtDu0UTO1dmEy-Oq&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 3: Network for Problem 3</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "Now you will implement a multi-layer network that has a hidden layer.  To start with a relatively simple case, we will do this without any non-linearities. The network has two input units, $x_1$ and $x_2$.  These are connected to a single hidden unit.  We'll call the activation of this hidden unit $h$, so $h = w_{11}x_1 + w_{12}x_2 + b_{11}$.  This hidden unit is connected to two output units.  We'll call their activation $z_1$ and $z_2$, so we have:\n",
        "\n",
        "$$z_1 = w_{21}h + b_{21}~~~~~~~~~~~z_2 = w_{22}h + b_{22}$$\n",
        "\n",
        "To train this network, we use a loss function that says that we want the output to be close to the input.  So the loss function is:  \n",
        "  \n",
        "$$L(x_1, x_2) = (z_1 - x_1)^2 + (z_2 - x_2)^2$$\n",
        "    \n",
        "That is, the input is also acting as the label.  This kind of network is called an {\\em auto-encoder}.  You may be wondering what the point of this is.  Because the hidden layer is smaller than the input and output layers, the network is forced to learn low-dimensional representation of the data.  In this case, the network learns to map the input points onto a line in the hidden layer, and then compute the 2D coordinates of the points on this line for the output layer.  This process is called Principal Component Analysis (PCA), and we will learn more about it later in the semester.\n",
        "\n",
        "We will provide a routine to generate training data:\n",
        "\n",
        "```pca_training_data(n, sigma)```  \n",
        "  \n",
        "The input parameter $n$ indicates the number of points in the training set.  As in the last problem, $X$ contains a $n \\times 2$ matrix in which each row contains the coordinates of a 2D point.  These points are generated to lie along the line $y = x + 1$.  Then Gaussian noise is added to the points, with zero mean and a standard deviation of sigma.\n",
        "\n",
        "Once again, you will implement training and testing routines.  \n",
        "  \n",
        "`pca_training(k, eta, n, sigma)`  \n",
        "  \n",
        "The input $k$ gives the number of iterations of gradient descent to use, while $eta$ gives the learning rate.  The input value $n$ indicates the number of points in the training set, while $sigma$ indicates the amount of noise added to these points.  Use these as parameters to pca\\_training\\_data.  The routine returns theta, a representation of all the weights and biases in the network.\n",
        "Also implement a test routine: \n",
        "\n",
        "```pca_test(theta, X)```  \n",
        "  \n",
        "$X$ will contain test data in the form returned by pca\\_training\\_data.  $Z$ provides the results the network produces given this input; $Z$ has the same format as $X$.  \n",
        "\n",
        "To test this, try training the network with $n = 10$ and $sigma = .1$.  Then test, using the input: `pca_test(theta, [[1,2], [4,5], [10, 3]])`.  \n",
        "  \n",
        "When I run my  code with this test I get: `[[0.9418, 2.0653], [3.9543, 5.0511], [6.1780, 7.2551]]`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI3FF-JptKqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9979f9-026f-491f-b120-c83ef4cd6e78"
      },
      "source": [
        "###Problem 3\n",
        "\n",
        "def partial_wrt_w2(z,x,h):\n",
        "  return (2*(z-x))*h\n",
        "\n",
        "def partial_wrt_w1(z1,x1,z2,x2,h,x):\n",
        "  p_wrt_z1 = (2*(z1-x1))\n",
        "  p_wrt_z2 = (2*(z2-x2))\n",
        "  return (p_wrt_z1 + p_wrt_z2) * x\n",
        "\n",
        "def partial_wrt_b2(z,x):\n",
        "  return (2*(z-x))\n",
        "\n",
        "def partial_wrt_b1(z1,x1,z2,x2):\n",
        "  p1 = (2*(z1-x1))\n",
        "  p2 = (2*(z2-x2))\n",
        "  return p1 + p2\n",
        "\n",
        "\n",
        "###Provided function to create training data\n",
        "def pca_training_data(n, sigma):\n",
        "    m = 1\n",
        "    b = 1\n",
        "    x1 = np.random.uniform(0,10,n)\n",
        "    x2 = m*x1+b\n",
        "    X = np.c_[x1, x2]\n",
        "    X += np.random.normal(0, sigma, X.shape)\n",
        "    return X\n",
        "\n",
        "def pca_training(k, eta, n, sigma):\n",
        "  X = pca_training_data(n, sigma)\n",
        "  n = len(X)\n",
        "  # initialize weights\n",
        "  w11 = np.random.normal()\n",
        "  w12 = np.random.normal()\n",
        "  w21 = np.random.normal()\n",
        "  w22 = np.random.normal()\n",
        "  w1 = np.asarray([w11,w12])\n",
        "  w2 = np.asarray([w21,w22])\n",
        "  w = np.asarray([w1,w2])\n",
        "\n",
        "  # initialize biases\n",
        "  b11 = 0\n",
        "  b21 = 0\n",
        "  b22 = 0\n",
        "  b = np.asarray([b11,b21,b22])\n",
        "\n",
        "  # initalize theta\n",
        "  theta = np.asarray([w,b])\n",
        "\n",
        "  for j in range(k):\n",
        "\n",
        "    p_wrt_w21 = 0\n",
        "    p_wrt_w22 = 0\n",
        "    p_wrt_w11 = 0\n",
        "    p_wrt_w12 = 0\n",
        "    p_wrt_b21 = 0\n",
        "    p_wrt_b22 = 0\n",
        "    p_wrt_b11 = 0\n",
        "\n",
        "    # iterate over training data\n",
        "    for i in range(n):\n",
        "\n",
        "      # get all the pieces\n",
        "      x = X[i]\n",
        "      x1 = x[0]\n",
        "      x2 = x[1]\n",
        "      w1 = theta[0][0]\n",
        "      w2 = theta[0][1]\n",
        "      w21 = w2[0]\n",
        "      w22 = w2[1]\n",
        "      b11 = theta[1][0]\n",
        "      b21 = theta[1][1]\n",
        "      b22 = theta[1][2]\n",
        "\n",
        "      # calculate hidden layer\n",
        "      h = np.dot(w1,x) + b11\n",
        "      \n",
        "      # calculate output\n",
        "      z1 = (w21 * h) + b21\n",
        "      z2 = (w22 * h) + b22\n",
        "\n",
        "      p_wrt_w21 += partial_wrt_w2(z1,x1,h)\n",
        "      p_wrt_w22 += partial_wrt_w2(z2,x2,h)\n",
        "      p_wrt_w11 += partial_wrt_w1(z1,x1,z2,x2,h,x1)\n",
        "      p_wrt_w12 += partial_wrt_w1(z1,x1,z2,x2,h,x2)\n",
        "      p_wrt_b21 += partial_wrt_b2(z1,x1)\n",
        "      p_wrt_b22 += partial_wrt_b2(z2,x2)\n",
        "      p_wrt_b11 += partial_wrt_b1(z1,x1,z2,x2)\n",
        "\n",
        "    # get the average and multiple by learning rate\n",
        "    w21_avg = eta * (p_wrt_w21/n)\n",
        "    w22_avg = eta * (p_wrt_w22/n)\n",
        "    w11_avg = eta * (p_wrt_w11/n)\n",
        "    w12_avg = eta * (p_wrt_w12/n)\n",
        "    b21_avg = eta * (p_wrt_b21/n)\n",
        "    b22_avg = eta * (p_wrt_b22/n)\n",
        "    b11_avg = eta * (p_wrt_b11/n)\n",
        "    \n",
        "    # build new theta\n",
        "    new_w1 = theta[0][0] - np.asarray([w11_avg,w12_avg])\n",
        "    new_w2 = theta[0][1] - np.asarray([w21_avg,w22_avg])\n",
        "    new_b11 = theta[1][0] - b11_avg\n",
        "    new_b21 = theta[1][1] - b21_avg\n",
        "    new_b22 = theta[1][2] - b22_avg\n",
        "\n",
        "    w = np.asarray([new_w1,new_w2])\n",
        "    b = np.asarray([new_b11,new_b21,new_b22])\n",
        "\n",
        "    # update theta\n",
        "    theta = np.asarray([w,b])\n",
        "  return theta\n",
        "\n",
        "def pca_test(theta, X):\n",
        "    Z = 0\n",
        "    n = len(X)\n",
        "    w1 = theta[0][0]\n",
        "    w2 = theta[0][1]\n",
        "    w21 = w2[0]\n",
        "    w22 = w2[1]\n",
        "    b11 = theta[1][0]\n",
        "    b21 = theta[1][1]\n",
        "    b22 = theta[1][2]\n",
        "    Z = []\n",
        "    for i in range(n):\n",
        "      h = np.dot(w1,X[i]) + b11\n",
        "      z1 = (w21 * h) + b21\n",
        "      z2 = (w22 * h) + b22\n",
        "      Z.append([z1,z2])\n",
        "    return Z\n",
        "\n",
        "print(\"TEST SIGMA = 0.1\")\n",
        "theta = pca_training(10000, 0.002, 10, 0.1)\n",
        "Y = pca_test(theta, [[1,2], [4,5], [10, 3]])\n",
        "print(Y)\n",
        "\n",
        "print(\"\\n TEST SIGMA = 0.0\")\n",
        "theta = pca_training(10000, 0.002, 10, 0.0)\n",
        "Y = pca_test(theta, [[1,2], [4,5], [10, 3]])\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST SIGMA = 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in subtract\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[nan, nan], [nan, nan], [nan, nan]]\n",
            "\n",
            " TEST SIGMA = 0.0\n",
            "[[1.0002378465088952, 1.9997621522284073], [4.000075178040519, 4.999924821563554], [8.376530695970654, 9.376854974038185]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwXLKsE9QQjt"
      },
      "source": [
        "### Analysis (10 Points)\n",
        "It may take a little work to find good values for $k$ and $\\eta$.  Add a description of your experimental results inside this cell.  Can you explain why the network produces the point $(6.1780, 7.2551)$ with an input of $(10, 3)$?\n",
        "\n",
        "Do another test with $sigma = 0$ instead of $sigma = .1$.  Run your network with the same test data.  How have the results changed?  Can you explain this change?\n",
        "- Upon first implementing the network, I was testing with a learning rate of 0.02. I noticed that the values of theta were swinging from very high values, to very low values, which indicated that it was \"bouncing around\" the minimum, and wound up diverging because of it. This was a strong hint that the learning rate was too high, and we need to take smaller steps down the gradient to reach the minimum. I lowered $\\eta$ to 0.002, and found that it not only converged, but produced a theta that gave reasonable output. I realized then, that I had been testing with a very small value of $k$ (so that it wound't take too long to traing the network). I bumped $k$ up to 10000, and found that the network produced good results. \n",
        "- The network produces (~6, ~7) with input (10,3) because of how the training data is being generated. The function generating the training data is $x2 = x1 + 1$ (with $sigma$ amount of noise added). This means that even though our network is supposed to be learning to return the original values $x1$ and $x2$, the weights and biases that it's learning to accomplish that are instead creating a linear combination of the two values in the hidden layer, and then separating them back out into numbers that are one apart. An inspection of the values of theta in one test run showed $w1 = 1.6$, $w2 = 1.3$, $b1 = 0.06$. This means that for $x = (1,2)$, $h =$ ~4.2. Then, $w3 = 0.35$, $w4 = 0.35$, $b2 = -0.47$ and $b3 = 0.53$. This means that the model will take about a third from $h$, and subtract a half to get $y1$, and add a half to get $y2$. We see here that $h$ is approximately halfway between $x1$ and $x2$, and since in all the training data $x1$ and $x2$ are approximately 1 apart, it learns to subtract half to get to $y1$, and add half to get to $y2$. When this procedure runs on (10,3), we get to the halfway point which is 6.5, and then subtract half, which gives ~6 and add half to get to ~7.\n",
        "- Running the network with a sigma of 0 give much tighter results, but they are essentially the same values. When sigma is 0, there is no additional noise added to the data, and so when given the same test data, the predicitons it makes are incredibly close to the actual values, when given $x1$ and $x2$ of the form $x2 = x1 + 1$. But we see that it is still learning the same function, because when given (10,3) it still produces values that are 1 apart, with $y1 < y2$  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wep92fjrcgK"
      },
      "source": [
        "## Problem 4 (optional challenge problem, for extra credit, 20 points):\n",
        "Ok, now you are ready to create a complete, fully connected neural net with a hidden layer and non-linearities. You will use this network to solve the XOR problem, using the same training data as in Problem 2. Your network architecture should have the following components:\n",
        "- Two input units, with activations $x_1$ and $x_2$.  These are just the coordinates of 2D points.\n",
        "- A variable number of hidden units, H.  Write your code so that you can select the number of hidden units as a hyperparameter.  Let's call the activation of the $i$'th hidden unit, $a^1_i$.  Let's call the weights of these units $w^1_{ij}$.  This is the weight from input unit $j$ to hidden unit $i$.  \n",
        "- se a RELU non-linearity for the hidden units.  So to determine the activation of a hidden unit we have: $z^1_i = w^1_{i1}x_1 + w^1_{i2}x_2 + b^1_i$, and $a^1_i = max(0, z^1_i)$.\n",
        "- There is then a single output unit.  Call its activation $a^2$.  We compute this as: $z^2 = \\left( \\sum_{i=1}^H w^2_{1i}a^1_i \\right) + b^2$, and $a^2 = \\sigma(z^2)$, where $\\sigma$ is the sigmoid nonlinearity.  This last part is just the same as in Problem 2.  And, like Problem 2, you can train your network using the cross-entropy loss.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/thumbnail?id=1qfLXZJsDFIwTfIdHxEP6HnJBmUrPGZsT&sz=w1000\" alt=\"net3\" width=\"400px\"/>\n",
        "<br>\n",
        "<i>Figure 4: Network for Problem 4</i>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Implement test and training functions with the templates:\n",
        "\n",
        "`nn_training(k, eta, trainset, H)`\n",
        "\n",
        "`nn_testing(theta, X)`\n",
        "\n",
        "The parameters to the training routine are similar to those in Problem 2, with $H$ indicating the number of hidden units.  The testing routine has the same form as in Problem 2.\n",
        "\n",
        "__Remember__: The `trainset` argument is the integer to be used to generate data with `single_layer_training_data(trainset)`, it is not the actual training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3U68nvJtM1_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9514607-280a-4208-ef94-f19efce018fb"
      },
      "source": [
        "###Problem 4: Challenge Problem\n",
        "def partial_ce(a,y):\n",
        "  n = (a - y)\n",
        "  d = (a * (1-a))\n",
        "  return (n/d)\n",
        "\n",
        "def partial_sigma(z):\n",
        "  return sigma(z) * (1 - sigma(z))\n",
        "\n",
        "def sigma(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "\n",
        "def nn_training(k, eta, trainset, H):\n",
        "  X,Y = single_layer_training_data(trainset)\n",
        "  n = len(X)\n",
        "  w1 = np.random.normal()\n",
        "  w2 = np.random.normal()\n",
        "  theta1 = []\n",
        "  theta2 = []\n",
        "  for i in range(H):\n",
        "    theta1.append(np.asarray([np.random.normal(),np.random.normal(),0]))\n",
        "    theta2.append(np.random.normal())\n",
        "  theta1 = np.asarray(theta1)\n",
        "  theta2 = np.asarray(theta2)\n",
        "  w2_g = np.asarray([])\n",
        "  w1_g = np.asarray([])\n",
        "\n",
        "  print(theta2)\n",
        "  for j in range(k):\n",
        "\n",
        "    # intialize acc for this epoch\n",
        "    acc = 0\n",
        "\n",
        "    for i in range(n):\n",
        "      # correct label\n",
        "      y = Y[i]\n",
        "\n",
        "      # x_i with 1 appended on the end to get bias\n",
        "      x = np.asarray([X[i][0],X[i][1],1])\n",
        "      \n",
        "      Z = []\n",
        "      A1 = []\n",
        "      for h in range(H):\n",
        "        Z.append(np.dot(theta1[h], x))\n",
        "        A1.append(sigma(np.dot(theta1[h], x)))\n",
        "      \n",
        "      Z2 = []\n",
        "      out = 0\n",
        "      for h in range(H):\n",
        "        Z2.append(theta2[h]*A1[h])\n",
        "\n",
        "      z = sum(Z2)\n",
        "      a2 = sigma(z)\n",
        "\n",
        "      w2_gradients = []\n",
        "      w1_gradients = []\n",
        "      for a in A1:\n",
        "        pl = partial_ce(a2,y)\n",
        "        pz = partial_sigma(z)\n",
        "        p_wrt_w = pl * pz * A1[h]\n",
        "        p_wrt_b = pl * pz\n",
        "        print(p_wrt_w)\n",
        "        w1_gradients.append(p_wrt_w)\n",
        "\n",
        "      \n",
        "      for i in range(len(theta2)):\n",
        "        wi2 = theta2[i]\n",
        "        pz = partial_sigma(Z[i])\n",
        "        p = partial_ce(a2,y) * partial_sigma(z) * wi2 * pz\n",
        "        w2_gradients.append([x[0]*p,x[1]*p])\n",
        "\n",
        "      w1_gradients = np.asarray(w1_gradients)\n",
        "      w2_gradients = np.asarray(w2_gradients)\n",
        "\n",
        "      if w1_g != np.asarray([]):\n",
        "        w1_g = w1_g + w1_gradients\n",
        "        w2_g = w2_g + w2_gradients\n",
        "      else:\n",
        "        w1_g = w1_gradients\n",
        "        w2_g = w2_gradients\n",
        "\n",
        "    # get the average\n",
        "    w1_g = w1_g/n\n",
        "    w2_g = w2_g/n\n",
        "    theta1[:,:2] = theta1[:,:2] - (eta * w1_g)\n",
        "    theta2 = theta2 - (eta * w2_g)\n",
        "  return 0 \n",
        "\n",
        "def nn_testing(theta, X):\n",
        "    #TODO: Your Code Here\n",
        "    return y\n",
        "\n",
        "nn_training(10, 0.001, 1, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.29384969 -0.09639677]\n",
            "-0.14746689754611084\n",
            "-0.14746689754611084\n",
            "-0.09010695623237652\n",
            "-0.09010695623237652\n",
            "-0.2268513842922609\n",
            "-0.2268513842922609\n",
            "-0.18634589642898525\n",
            "-0.18634589642898525\n",
            "-0.2620224371076494\n",
            "-0.2620224371076494\n",
            "-0.2626628361859717\n",
            "-0.2626628361859717\n",
            "-0.06392495958850539\n",
            "-0.06392495958850539\n",
            "-0.12069720129677912\n",
            "-0.12069720129677912\n",
            "-0.24006775272442127\n",
            "-0.24006775272442127\n",
            "-0.14571661688094206\n",
            "-0.14571661688094206\n",
            "0.043402025184854275\n",
            "0.043402025184854275\n",
            "0.012077001465047829\n",
            "0.012077001465047829\n",
            "0.03825234825357959\n",
            "0.03825234825357959\n",
            "0.002743984421246163\n",
            "0.002743984421246163\n",
            "0.016107220553720394\n",
            "0.016107220553720394\n",
            "0.00568692919632469\n",
            "0.00568692919632469\n",
            "0.002229497607357915\n",
            "0.002229497607357915\n",
            "0.004532853009012558\n",
            "0.004532853009012558\n",
            "0.0008897824355599491\n",
            "0.0008897824355599491\n",
            "0.0060939112164928805\n",
            "0.0060939112164928805\n",
            "[-0.10420071 -0.19416728]\n",
            "[-0.10420071 -0.19416728]\n",
            "[-0.07236521 -0.12310775]\n",
            "[-0.07236521 -0.12310775]\n",
            "[-0.13965772 -0.27539725]\n",
            "[-0.13965772 -0.27539725]\n",
            "[-0.12522296 -0.22796073]\n",
            "[-0.12522296 -0.22796073]\n",
            "[-0.08946894 -0.50748858]\n",
            "[-0.08946894 -0.50748858]\n",
            "[-0.11709179 -0.40649521]\n",
            "[-0.11709179 -0.40649521]\n",
            "[-0.05362689 -0.09563121]\n",
            "[-0.05362689 -0.09563121]\n",
            "[-0.08702943 -0.17598901]\n",
            "[-0.08702943 -0.17598901]\n",
            "[-0.08495041 -0.48739142]\n",
            "[-0.08495041 -0.48739142]\n",
            "[-0.08441998 -0.26800321]\n",
            "[-0.08441998 -0.26800321]\n",
            "[0.04418237 0.02631996]\n",
            "[0.04418237 0.02631996]\n",
            "[0.01215154 0.00733192]\n",
            "[0.01215154 0.00733192]\n",
            "[0.03885533 0.02319474]\n",
            "[0.03885533 0.02319474]\n",
            "[0.00275113 0.00166643]\n",
            "[0.00275113 0.00166643]\n",
            "[0.01623177 0.0097775 ]\n",
            "[0.01623177 0.0097775 ]\n",
            "[0.00570886 0.00345374]\n",
            "[0.00570886 0.00345374]\n",
            "[0.00223491 0.00135403]\n",
            "[0.00223491 0.00135403]\n",
            "[0.00454873 0.00275321]\n",
            "[0.00454873 0.00275321]\n",
            "[0.00089149 0.00054042]\n",
            "[0.00089149 0.00054042]\n",
            "[0.00611629 0.00369963]\n",
            "[0.00611629 0.00369963]\n",
            "[[-0.10420121 -0.19415829]\n",
            " [-0.10420208 -0.19415745]]\n",
            "[[-0.10420121 -0.19415829]\n",
            " [-0.10420208 -0.19415745]]\n",
            "[[-0.07236476 -0.12310355]\n",
            " [-0.07236521 -0.12310309]]\n",
            "[[-0.07236476 -0.12310355]\n",
            " [-0.07236521 -0.12310309]]\n",
            "[[-0.13966076 -0.27537868]\n",
            " [-0.13966221 -0.27537737]]\n",
            "[[-0.13966076 -0.27537868]\n",
            " [-0.13966221 -0.27537737]]\n",
            "[[-0.12522335 -0.22794352]\n",
            " [-0.12522444 -0.22794254]]\n",
            "[[-0.12522335 -0.22794352]\n",
            " [-0.12522444 -0.22794254]]\n",
            "[[-0.08947129 -0.5074968 ]\n",
            " [-0.08947356 -0.50749184]]\n",
            "[[-0.08947129 -0.5074968 ]\n",
            " [-0.08947356 -0.50749184]]\n",
            "[[-0.11709655 -0.40649884]\n",
            " [-0.11709872 -0.40649574]]\n",
            "[[-0.11709655 -0.40649884]\n",
            " [-0.11709872 -0.40649574]]\n",
            "[[-0.05362765 -0.09563247]\n",
            " [-0.05362799 -0.09563209]]\n",
            "[[-0.05362765 -0.09563247]\n",
            " [-0.05362799 -0.09563209]]\n",
            "[[-0.08703079 -0.17598861]\n",
            " [-0.08703157 -0.17598777]]\n",
            "[[-0.08703079 -0.17598861]\n",
            " [-0.08703157 -0.17598777]]\n",
            "[[-0.08495228 -0.48740501]\n",
            " [-0.08495441 -0.48740024]]\n",
            "[[-0.08495228 -0.48740501]\n",
            " [-0.08495441 -0.48740024]]\n",
            "[[-0.08442253 -0.26802332]\n",
            " [-0.0844238  -0.26802142]]\n",
            "[[-0.08442253 -0.26802332]\n",
            " [-0.0844238  -0.26802142]]\n",
            "[[0.0442727  0.02637282]\n",
            " [0.0442726  0.02637299]]\n",
            "[[0.0442727  0.02637282]\n",
            " [0.0442726  0.02637299]]\n",
            "[[0.01217376 0.00734525]\n",
            " [0.01217374 0.0073453 ]]\n",
            "[[0.01217376 0.00734525]\n",
            " [0.01217374 0.0073453 ]]\n",
            "[[0.03891874 0.023232  ]\n",
            " [0.03891866 0.02323214]]\n",
            "[[0.03891874 0.023232  ]\n",
            " [0.03891866 0.02323214]]\n",
            "[[0.00275581 0.00166926]\n",
            " [0.0027558  0.00166927]]\n",
            "[[0.00275581 0.00166926]\n",
            " [0.0027558  0.00166927]]\n",
            "[[0.01626256 0.00979592]\n",
            " [0.01626253 0.00979598]]\n",
            "[[0.01626256 0.00979592]\n",
            " [0.01626253 0.00979598]]\n",
            "[[0.00571983 0.00346036]\n",
            " [0.00571982 0.00346038]]\n",
            "[[0.00571983 0.00346036]\n",
            " [0.00571982 0.00346038]]\n",
            "[[0.00223874 0.00135635]\n",
            " [0.00223873 0.00135636]]\n",
            "[[0.00223874 0.00135635]\n",
            " [0.00223873 0.00135636]]\n",
            "[[0.0045579  0.00275875]\n",
            " [0.0045579  0.00275877]]\n",
            "[[0.0045579  0.00275875]\n",
            " [0.0045579  0.00275877]]\n",
            "[[0.00089297 0.00054132]\n",
            " [0.00089297 0.00054132]]\n",
            "[[0.00089297 0.00054132]\n",
            " [0.00089297 0.00054132]]\n",
            "[[0.00612534 0.00370509]\n",
            " [0.00612533 0.00370511]]\n",
            "[[0.00612534 0.00370509]\n",
            " [0.00612533 0.00370511]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-cb89e92c1c3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mnn_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-cb89e92c1c3a>\u001b[0m in \u001b[0;36mnn_training\u001b[0;34m(k, eta, trainset, H)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mw1_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw1_g\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mw2_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2_g\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtheta1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw1_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtheta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw2_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,2,2) into shape (2,2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJcDH94VQ3iV"
      },
      "source": [
        "### Analysis (5 Points)\n",
        "Run experiments to demonstrate that your network can solve the XOR problem. How do you find the results vary as you vary the number of hidden units?  Show and discuss the results of your experiments inside/below this cell.\n",
        "\n",
        "- !!! _YOUR RESPONSE HERE_ !!!"
      ]
    }
  ]
}